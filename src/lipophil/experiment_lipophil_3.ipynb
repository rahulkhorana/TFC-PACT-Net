{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqYoTBJrCcJx",
        "outputId": "61bbe9af-38c5-4d63-c8fa-6dab90559823"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (2025.3.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.7.14)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.1.2->aiohttp->torch_geometric) (4.14.1)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.11/dist-packages (2.1.2+pt26cu124)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric rdkit\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__)\").html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M0suAYOT5R-N"
      },
      "outputs": [],
      "source": [
        "# Standard library\n",
        "from collections import defaultdict\n",
        "from typing import List, Tuple\n",
        "import importlib.resources as pkg_resources\n",
        "from multiprocessing.pool import ThreadPool\n",
        "\n",
        "# Third-party scientific stack\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.sparse import coo_matrix\n",
        "from scipy.sparse.linalg import eigsh\n",
        "\n",
        "# RDKit\n",
        "from rdkit import Chem\n",
        "\n",
        "# PyTorch core\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# PyTorch Geometric\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, Batch, DataLoader\n",
        "from torch_geometric.nn import GCNConv, GINConv, BatchNorm, global_mean_pool\n",
        "from torch_geometric.loader import DataLoader as PyGDataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Wginy3UChb_",
        "outputId": "f8a7c1a0-54a6-40d2-9be6-9f7a618a90fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "obXQxOveCM9l"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Fix all random seeds for reproducibility across Python, NumPy, and PyTorch.\n",
        "    \"\"\"\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(59)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fYyEoAXuCM9m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import (\n",
        "    PNAConv,\n",
        "    global_mean_pool,\n",
        "    global_max_pool,\n",
        "    global_add_pool\n",
        ")\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "class PolyatomicNet(nn.Module):\n",
        "    def __init__(self, node_feat_dim, edge_feat_dim, graph_feat_dim,deg,\n",
        "                 hidden_dim=128, num_layers=5, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.node_emb = nn.Linear(node_feat_dim, hidden_dim)\n",
        "        self.deg = deg\n",
        "        self.virtualnode_emb = nn.Embedding(1, hidden_dim)\n",
        "        self.vn_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # For graph-level feature projection\n",
        "        self.graph_proj = nn.Sequential(\n",
        "            nn.Linear(graph_feat_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # PNAConv requires degree preprocessing\n",
        "        self.deg_emb = nn.Embedding(20, hidden_dim)  # cap degree buckets\n",
        "\n",
        "        aggregators = ['mean', 'min', 'max', 'std']\n",
        "        scalers = ['identity', 'amplification', 'attenuation']\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.bns = nn.ModuleList()\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            conv = PNAConv(\n",
        "                in_channels=hidden_dim,\n",
        "                out_channels=hidden_dim,\n",
        "                aggregators=aggregators,\n",
        "                scalers=scalers,\n",
        "                edge_dim=edge_feat_dim,\n",
        "                towers=4,\n",
        "                pre_layers=1,\n",
        "                post_layers=1,\n",
        "                divide_input=True,\n",
        "                deg=deg\n",
        "            )\n",
        "            self.convs.append(conv)\n",
        "            self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Final readout\n",
        "        self.readout = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 3, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, batch = (\n",
        "            data.x,\n",
        "            data.edge_index,\n",
        "            data.edge_attr,\n",
        "            data.batch,\n",
        "        )\n",
        "\n",
        "        deg = degree(edge_index[0], x.size(0), dtype=torch.long).clamp(max=19)\n",
        "        h = self.node_emb(x) + self.deg_emb(deg)\n",
        "\n",
        "        vn = self.virtualnode_emb(\n",
        "            torch.zeros(batch.max().item() + 1, dtype=torch.long, device=x.device)\n",
        "        )\n",
        "\n",
        "        for conv, bn in zip(self.convs, self.bns):\n",
        "            h = h + vn[batch]\n",
        "            h = conv(h, edge_index, edge_attr)\n",
        "            h = bn(h)\n",
        "            h = F.relu(h)\n",
        "            h = self.dropout(h)\n",
        "            vn = vn + self.vn_mlp(global_mean_pool(h, batch))\n",
        "\n",
        "        mean_pool = global_mean_pool(h, batch)\n",
        "        max_pool  = global_max_pool(h, batch)\n",
        "        add_pool  = global_add_pool(h, batch)\n",
        "\n",
        "        # graph_feats shape: [batch_size, graph_feat_dim]\n",
        "        if hasattr(data, 'graph_feats') and isinstance(data, torch_geometric.data.Batch):\n",
        "            g_feats = torch.stack([g.graph_feats for g in data.to_data_list()], dim=0).to(x.device)\n",
        "        else:\n",
        "            g_feats = data.graph_feats.unsqueeze(0).to(x.device)  # handles single graph batch\n",
        "        g_proj = self.graph_proj(g_feats)\n",
        "\n",
        "        final_input = torch.cat([mean_pool, max_pool, g_proj], dim=1)\n",
        "        return self.readout(final_input).view(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "daTRiHSFCM9m"
      },
      "outputs": [],
      "source": [
        "from torch.amp import autocast, GradScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "scaler_grad = GradScaler()\n",
        "\n",
        "def train(model, loader, optimizer, loss_fn, scaler=scaler_grad, accum_steps=8):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, batch in enumerate(loader):\n",
        "        batch = batch.to(device)\n",
        "        with autocast(device_type='cuda', dtype=torch.float16):\n",
        "            output = model(batch)\n",
        "            loss = loss_fn(output, batch.y.view(-1)) / accum_steps\n",
        "\n",
        "        scaler_grad.scale(loss).backward()\n",
        "\n",
        "        if (i + 1) % accum_steps == 0 or (i + 1 == len(loader)):\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * batch.num_graphs * accum_steps\n",
        "\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def compute_metrics_with_ci(trues, preds, n_boot=2000, alpha=0.05, seed=42):\n",
        "    trues = np.array(trues)\n",
        "    preds = np.array(preds)\n",
        "    mae = mean_absolute_error(trues, preds)\n",
        "    rmse = np.sqrt(mean_squared_error(trues, preds))\n",
        "\n",
        "    rng = np.random.RandomState(seed)\n",
        "    mae_samples, rmse_samples = [], []\n",
        "    n = len(trues)\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.randint(0, n, n)\n",
        "        t, p = trues[idx], preds[idx]\n",
        "        mae_samples.append(mean_absolute_error(t, p))\n",
        "        rmse_samples.append(np.sqrt(mean_squared_error(t, p)))\n",
        "\n",
        "    lower, upper = 100 * alpha / 2, 100 * (1 - alpha / 2)\n",
        "    mae_ci = (np.percentile(mae_samples, lower), np.percentile(mae_samples, upper))\n",
        "    rmse_ci = (np.percentile(rmse_samples, lower), np.percentile(rmse_samples, upper))\n",
        "    return {'mae': mae, 'mae_ci': mae_ci, 'rmse': rmse, 'rmse_ci': rmse_ci}\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    preds, trues = [], []\n",
        "    with torch.no_grad(), autocast(device_type='cuda', dtype=torch.float16):\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch)\n",
        "            preds.append(out.view(-1))\n",
        "            trues.append(batch.y.view(-1))\n",
        "    preds = torch.cat(preds)\n",
        "    trues = torch.cat(trues)\n",
        "    return torch.sqrt(torch.mean((preds - trues) ** 2)).item()\n",
        "\n",
        "def evaluate_with_ci(model, loader):\n",
        "    model.eval()\n",
        "    preds, trues = [], []\n",
        "    with torch.no_grad(), autocast(device_type='cuda', dtype=torch.float16):\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch)\n",
        "            y_true = batch.y.view(-1).cpu().tolist()\n",
        "            y_pred = out.view(-1).cpu().tolist()\n",
        "            trues.extend(y_true)\n",
        "            preds.extend(y_pred)\n",
        "    return compute_metrics_with_ci(trues, preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b7jK5FEjCM9m"
      },
      "outputs": [],
      "source": [
        "loading = True\n",
        "if loading:\n",
        "    all_data = torch.load('/content/drive/MyDrive/richer_lipophil_data.pt', weights_only=False)\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    ys = np.array([d.y.item() for d in all_data]).reshape(-1, 1)\n",
        "    ys_scaled = scaler.fit_transform(ys)\n",
        "    for i, d in enumerate(all_data):\n",
        "        d.y = torch.tensor([ys_scaled[i][0]], dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aT9Ljr-SgTsE"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*An output with one or more elements was resized.*\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*FutureWarning:.*\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.optim.lr_scheduler\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_ZBbQn-98XEu"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "data_list = all_data\n",
        "train_n = int(0.8 * len(data_list))\n",
        "train_ds, test_ds = torch.utils.data.random_split(\n",
        "    data_list, [train_n, len(data_list)-train_n]\n",
        ")\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=8, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=128)\n",
        "\n",
        "node_feat_dim = all_data[0].x.shape[1]\n",
        "edge_feat_dim = all_data[0].edge_attr.shape[1]\n",
        "graph_feat_dim = all_data[0].graph_feats.shape[0]\n",
        "\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "deg = torch.zeros(128, dtype=torch.long)  # increase if needed\n",
        "\n",
        "for data in train_ds:\n",
        "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
        "    bc = torch.bincount(d, minlength=deg.size(0))\n",
        "    if bc.size(0) > deg.size(0):\n",
        "        # Expand deg to fit\n",
        "        new_deg = torch.zeros(bc.size(0), dtype=torch.long)\n",
        "        new_deg[:deg.size(0)] = deg\n",
        "        deg = new_deg\n",
        "    deg += bc\n",
        "\n",
        "model = PolyatomicNet(\n",
        "    node_feat_dim=node_feat_dim,\n",
        "    edge_feat_dim=edge_feat_dim,\n",
        "    graph_feat_dim=graph_feat_dim,\n",
        "    deg=deg,\n",
        ")\n",
        "\n",
        "input_dim = all_data[0].x.size(1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', patience=10, factor=0.5, verbose=True\n",
        ")\n",
        "loss_fn   = nn.SmoothL1Loss(beta=0.5)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XchdS-ZoCM9m",
        "outputId": "382edeff-1ab7-4089-958e-30485b94d7c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | Train Loss: 0.5892 | Test MAE: 0.8077 (95% CI [0.7672, 0.8486]) | Test RMSE: 1.0018 (95% CI [0.9553, 1.0520])\n",
            "Epoch 02 | Train Loss: 0.5659 | Test MAE: 0.7664 (95% CI [0.7279, 0.8079]) | Test RMSE: 0.9556 (95% CI [0.9095, 1.0029])\n",
            "Epoch 03 | Train Loss: 0.5348 | Test MAE: 0.7660 (95% CI [0.7295, 0.8054]) | Test RMSE: 0.9485 (95% CI [0.9025, 0.9952])\n",
            "Epoch 04 | Train Loss: 0.5156 | Test MAE: 0.8529 (95% CI [0.8124, 0.8941]) | Test RMSE: 1.0300 (95% CI [0.9859, 1.0739])\n",
            "Epoch 05 | Train Loss: 0.5062 | Test MAE: 0.7375 (95% CI [0.7020, 0.7744]) | Test RMSE: 0.9085 (95% CI [0.8647, 0.9533])\n",
            "Epoch 06 | Train Loss: 0.4855 | Test MAE: 0.7023 (95% CI [0.6659, 0.7397]) | Test RMSE: 0.8833 (95% CI [0.8361, 0.9314])\n",
            "Epoch 07 | Train Loss: 0.4656 | Test MAE: 0.7058 (95% CI [0.6663, 0.7460]) | Test RMSE: 0.9035 (95% CI [0.8511, 0.9578])\n",
            "Epoch 08 | Train Loss: 0.4829 | Test MAE: 0.7042 (95% CI [0.6676, 0.7427]) | Test RMSE: 0.8870 (95% CI [0.8418, 0.9336])\n",
            "Epoch 09 | Train Loss: 0.4698 | Test MAE: 0.6838 (95% CI [0.6474, 0.7210]) | Test RMSE: 0.8663 (95% CI [0.8202, 0.9156])\n",
            "Epoch 10 | Train Loss: 0.4474 | Test MAE: 0.6923 (95% CI [0.6532, 0.7320]) | Test RMSE: 0.8992 (95% CI [0.8485, 0.9522])\n",
            "Epoch 11 | Train Loss: 0.4384 | Test MAE: 0.7631 (95% CI [0.7246, 0.8016]) | Test RMSE: 0.9445 (95% CI [0.9008, 0.9871])\n",
            "Epoch 12 | Train Loss: 0.4329 | Test MAE: 0.8079 (95% CI [0.7628, 0.8535]) | Test RMSE: 1.0422 (95% CI [0.9881, 1.0960])\n",
            "Epoch 13 | Train Loss: 0.4293 | Test MAE: 0.6875 (95% CI [0.6520, 0.7250]) | Test RMSE: 0.8729 (95% CI [0.8259, 0.9201])\n",
            "Epoch 14 | Train Loss: 0.4155 | Test MAE: 0.8605 (95% CI [0.8211, 0.9006]) | Test RMSE: 1.0431 (95% CI [0.9974, 1.0885])\n",
            "Epoch 15 | Train Loss: 0.3996 | Test MAE: 0.6477 (95% CI [0.6135, 0.6839]) | Test RMSE: 0.8283 (95% CI [0.7820, 0.8746])\n",
            "Epoch 16 | Train Loss: 0.3909 | Test MAE: 0.7151 (95% CI [0.6791, 0.7496]) | Test RMSE: 0.8904 (95% CI [0.8437, 0.9358])\n",
            "Epoch 17 | Train Loss: 0.3938 | Test MAE: 0.7160 (95% CI [0.6772, 0.7594]) | Test RMSE: 0.9306 (95% CI [0.8802, 0.9840])\n",
            "Epoch 18 | Train Loss: 0.3876 | Test MAE: 0.8033 (95% CI [0.7656, 0.8424]) | Test RMSE: 0.9857 (95% CI [0.9394, 1.0344])\n",
            "Epoch 19 | Train Loss: 0.3735 | Test MAE: 0.5989 (95% CI [0.5654, 0.6339]) | Test RMSE: 0.7818 (95% CI [0.7346, 0.8305])\n",
            "Epoch 20 | Train Loss: 0.3506 | Test MAE: 0.6417 (95% CI [0.6046, 0.6805]) | Test RMSE: 0.8503 (95% CI [0.7956, 0.9096])\n",
            "Epoch 21 | Train Loss: 0.3515 | Test MAE: 0.6096 (95% CI [0.5768, 0.6443]) | Test RMSE: 0.7797 (95% CI [0.7348, 0.8268])\n",
            "Epoch 22 | Train Loss: 0.3464 | Test MAE: 0.7709 (95% CI [0.7313, 0.8133]) | Test RMSE: 0.9752 (95% CI [0.9245, 1.0290])\n",
            "Epoch 23 | Train Loss: 0.3346 | Test MAE: 0.6047 (95% CI [0.5701, 0.6403]) | Test RMSE: 0.7874 (95% CI [0.7395, 0.8378])\n",
            "Epoch 24 | Train Loss: 0.3357 | Test MAE: 0.6739 (95% CI [0.6364, 0.7143]) | Test RMSE: 0.8722 (95% CI [0.8210, 0.9219])\n",
            "Epoch 25 | Train Loss: 0.3173 | Test MAE: 0.9726 (95% CI [0.9330, 1.0118]) | Test RMSE: 1.1470 (95% CI [1.1042, 1.1891])\n",
            "Epoch 26 | Train Loss: 0.3213 | Test MAE: 0.6857 (95% CI [0.6479, 0.7238]) | Test RMSE: 0.8881 (95% CI [0.8429, 0.9354])\n",
            "Epoch 27 | Train Loss: 0.3284 | Test MAE: 0.5864 (95% CI [0.5511, 0.6244]) | Test RMSE: 0.7823 (95% CI [0.7330, 0.8321])\n",
            "Epoch 28 | Train Loss: 0.3232 | Test MAE: 0.5646 (95% CI [0.5313, 0.6010]) | Test RMSE: 0.7503 (95% CI [0.7035, 0.8018])\n",
            "Epoch 29 | Train Loss: 0.3014 | Test MAE: 0.6449 (95% CI [0.6113, 0.6776]) | Test RMSE: 0.8113 (95% CI [0.7670, 0.8535])\n",
            "Epoch 30 | Train Loss: 0.2923 | Test MAE: 0.5286 (95% CI [0.4969, 0.5609]) | Test RMSE: 0.7032 (95% CI [0.6546, 0.7532])\n",
            "Epoch 31 | Train Loss: 0.2842 | Test MAE: 0.5337 (95% CI [0.5018, 0.5681]) | Test RMSE: 0.7130 (95% CI [0.6638, 0.7633])\n",
            "Epoch 32 | Train Loss: 0.2805 | Test MAE: 0.5377 (95% CI [0.5068, 0.5689]) | Test RMSE: 0.7074 (95% CI [0.6603, 0.7539])\n",
            "Epoch 33 | Train Loss: 0.2842 | Test MAE: 0.8389 (95% CI [0.8020, 0.8753]) | Test RMSE: 1.0010 (95% CI [0.9570, 1.0442])\n",
            "Epoch 34 | Train Loss: 0.2985 | Test MAE: 0.5405 (95% CI [0.5079, 0.5743]) | Test RMSE: 0.7190 (95% CI [0.6694, 0.7705])\n",
            "Epoch 35 | Train Loss: 0.2847 | Test MAE: 0.5763 (95% CI [0.5420, 0.6131]) | Test RMSE: 0.7732 (95% CI [0.7236, 0.8269])\n",
            "Epoch 36 | Train Loss: 0.2802 | Test MAE: 0.5210 (95% CI [0.4904, 0.5546]) | Test RMSE: 0.6973 (95% CI [0.6510, 0.7464])\n",
            "Epoch 37 | Train Loss: 0.2773 | Test MAE: 0.5320 (95% CI [0.5009, 0.5657]) | Test RMSE: 0.7081 (95% CI [0.6605, 0.7590])\n",
            "Epoch 38 | Train Loss: 0.2771 | Test MAE: 0.4973 (95% CI [0.4666, 0.5309]) | Test RMSE: 0.6722 (95% CI [0.6245, 0.7209])\n",
            "Epoch 39 | Train Loss: 0.2634 | Test MAE: 0.4925 (95% CI [0.4627, 0.5237]) | Test RMSE: 0.6634 (95% CI [0.6149, 0.7128])\n",
            "Epoch 40 | Train Loss: 0.2551 | Test MAE: 0.5561 (95% CI [0.5241, 0.5906]) | Test RMSE: 0.7267 (95% CI [0.6788, 0.7767])\n",
            "Epoch 41 | Train Loss: 0.2582 | Test MAE: 0.5224 (95% CI [0.4918, 0.5539]) | Test RMSE: 0.6899 (95% CI [0.6428, 0.7411])\n",
            "Epoch 42 | Train Loss: 0.2534 | Test MAE: 0.5052 (95% CI [0.4742, 0.5379]) | Test RMSE: 0.6845 (95% CI [0.6330, 0.7366])\n",
            "Epoch 43 | Train Loss: 0.2512 | Test MAE: 0.6180 (95% CI [0.5846, 0.6525]) | Test RMSE: 0.7955 (95% CI [0.7490, 0.8450])\n",
            "Epoch 44 | Train Loss: 0.2453 | Test MAE: 0.4957 (95% CI [0.4661, 0.5273]) | Test RMSE: 0.6692 (95% CI [0.6217, 0.7187])\n",
            "Epoch 45 | Train Loss: 0.2443 | Test MAE: 0.5061 (95% CI [0.4751, 0.5405]) | Test RMSE: 0.6820 (95% CI [0.6302, 0.7340])\n",
            "Epoch 46 | Train Loss: 0.2427 | Test MAE: 0.6276 (95% CI [0.5922, 0.6638]) | Test RMSE: 0.8265 (95% CI [0.7777, 0.8779])\n",
            "Epoch 47 | Train Loss: 0.2494 | Test MAE: 0.5009 (95% CI [0.4718, 0.5332]) | Test RMSE: 0.6647 (95% CI [0.6189, 0.7126])\n",
            "Epoch 48 | Train Loss: 0.2385 | Test MAE: 0.4787 (95% CI [0.4493, 0.5093]) | Test RMSE: 0.6497 (95% CI [0.6038, 0.6984])\n",
            "Epoch 49 | Train Loss: 0.2348 | Test MAE: 0.4836 (95% CI [0.4560, 0.5144]) | Test RMSE: 0.6461 (95% CI [0.6007, 0.6922])\n"
          ]
        }
      ],
      "source": [
        "history = {}\n",
        "N = 50\n",
        "for epoch in range(1, N):\n",
        "    tr_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    metrics = evaluate_with_ci(model, test_loader)\n",
        "    print(f\"Epoch {epoch:02d} | Train Loss: {tr_loss:.4f} | \"\n",
        "          f\"Test MAE: {metrics['mae']:.4f} (95% CI [{metrics['mae_ci'][0]:.4f}, {metrics['mae_ci'][1]:.4f}]) | \"\n",
        "          f\"Test RMSE: {metrics['rmse']:.4f} (95% CI [{metrics['rmse_ci'][0]:.4f}, {metrics['rmse_ci'][1]:.4f}])\")\n",
        "    history[epoch] = metrics\n",
        "    scheduler.step(metrics['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhNtZfkfCM9m",
        "outputId": "525b9241-f9de-4f39-e4e9-e6dbad244408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "********************\n",
            "Test MAE: 0.4836 (95% CI [0.4560, 0.5144])\n",
            "Test RMSE: 0.6461 (95% CI [0.6007, 0.6922])\n",
            "********************\n"
          ]
        }
      ],
      "source": [
        "final = history[N-1]\n",
        "print(\"*\"*20)\n",
        "print(f\"Test MAE: {final['mae']:.4f} (95% CI [{final['mae_ci'][0]:.4f}, {final['mae_ci'][1]:.4f}])\")\n",
        "print(f\"Test RMSE: {final['rmse']:.4f} (95% CI [{final['rmse_ci'][0]:.4f}, {final['rmse_ci'][1]:.4f}])\")\n",
        "print(\"*\"*20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4QDfZgRTaEak"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
